Cloning into 'examples'...
Warning: Permanently added 'github.com,140.82.112.4' (ECDSA) to the list of known hosts.
Collecting mosaicml@ git+https://github.com/bmosaicml/composer.git@feature/fewshot_lambada
  Cloning https://github.com/bmosaicml/composer.git (to revision feature/fewshot_lambada) to /tmp/pip-install-86vkvz2d/mosaicml_cde2889706a44b94867179d2411af24a
  Running command git clone --filter=blob:none --quiet https://github.com/bmosaicml/composer.git /tmp/pip-install-86vkvz2d/mosaicml_cde2889706a44b94867179d2411af24a
  Running command git checkout -b feature/fewshot_lambada --track origin/feature/fewshot_lambada
  Switched to a new branch 'feature/fewshot_lambada'
  branch 'feature/fewshot_lambada' set up to track 'origin/feature/fewshot_lambada'.
  Resolved https://github.com/bmosaicml/composer.git to commit ce4ab206ab0a45aeee182fba0bda15a14fa35ca1
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting lm-eval@ git+https://github.com/bmosaicml/lm-evaluation-harness-composergpt-integration.git@master
  Cloning https://github.com/bmosaicml/lm-evaluation-harness-composergpt-integration.git (to revision master) to /tmp/pip-install-86vkvz2d/lm-eval_c01ed6efdb6c4e18b288857918abf42d
  Running command git clone --filter=blob:none --quiet https://github.com/bmosaicml/lm-evaluation-harness-composergpt-integration.git /tmp/pip-install-86vkvz2d/lm-eval_c01ed6efdb6c4e18b288857918abf42d
  Resolved https://github.com/bmosaicml/lm-evaluation-harness-composergpt-integration.git to commit 818a1587aafc2c8c9964268b50c611cee1583b7c
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: torchvision==0.14.0 in /usr/lib/python3/dist-packages (from -r llm/requirements.txt (line 1)) (0.14.0+cu117)
Requirement already satisfied: torchtext==0.14.0 in /usr/lib/python3/dist-packages (from -r llm/requirements.txt (line 2)) (0.14.0)
Requirement already satisfied: torch==1.13.0 in /usr/lib/python3/dist-packages (from -r llm/requirements.txt (line 3)) (1.13.0+cu117)
Collecting mosaicml-streaming==0.2.2
  Downloading mosaicml_streaming-0.2.2-py3-none-any.whl (108 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 108.4/108.4 kB 5.4 MB/s eta 0:00:00
Requirement already satisfied: flash-attn==0.2.2 in /usr/lib/python3/dist-packages (from -r llm/requirements.txt (line 6)) (0.2.2)
Collecting triton==2.0.0.dev20221103
  Downloading triton-2.0.0.dev20221103-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/18.7 MB 143.7 MB/s eta 0:00:00
Collecting transformers==4.24.0
  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 134.8 MB/s eta 0:00:00
Collecting datasets==2.7.1
  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 451.7/451.7 kB 141.4 MB/s eta 0:00:00
Collecting omegaconf==2.2.3
  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.3/79.3 kB 52.5 MB/s eta 0:00:00
Collecting wandb==0.13.4
  Downloading wandb-0.13.4-py2.py3-none-any.whl (1.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.9/1.9 MB 129.9 MB/s eta 0:00:00
Requirement already satisfied: pytest in /usr/lib/python3/dist-packages (from -r llm/requirements.txt (line 14)) (7.2.0)
Collecting pandas
  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.1/12.1 MB 182.6 MB/s eta 0:00:00
Requirement already satisfied: typing-extensions in /usr/lib/python3/dist-packages (from torchvision==0.14.0->-r llm/requirements.txt (line 1)) (4.4.0)
Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision==0.14.0->-r llm/requirements.txt (line 1)) (7.0.0)
Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from torchvision==0.14.0->-r llm/requirements.txt (line 1)) (2.22.0)
Requirement already satisfied: numpy in /usr/lib/python3/dist-packages (from torchvision==0.14.0->-r llm/requirements.txt (line 1)) (1.24.1)
Requirement already satisfied: tqdm in /usr/lib/python3/dist-packages (from torchtext==0.14.0->-r llm/requirements.txt (line 2)) (4.64.1)
Collecting matplotlib<4,>=3.5.2
  Downloading matplotlib-3.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.8/11.8 MB 181.1 MB/s eta 0:00:00
Collecting python-snappy<1,>=0.6.1
  Downloading python_snappy-0.6.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (55 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.9/55.9 kB 38.0 MB/s eta 0:00:00
Collecting oci<3,>=2.88
  Downloading oci-2.90.3-py2.py3-none-any.whl (18.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.7/18.7 MB 154.4 MB/s eta 0:00:00
Collecting boto3<2,>=1.21.45
  Downloading boto3-1.26.57-py3-none-any.whl (132 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 132.7/132.7 kB 70.2 MB/s eta 0:00:00
Collecting xxhash<4,>=3.0.0
  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 212.5/212.5 kB 102.3 MB/s eta 0:00:00
Collecting zstd<2,>=1.5.2.5
  Downloading zstd-1.5.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 191.5 MB/s eta 0:00:00
Collecting paramiko<3,>=2.11.0
  Downloading paramiko-2.12.0-py2.py3-none-any.whl (213 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 213.1/213.1 kB 106.0 MB/s eta 0:00:00
Collecting Brotli>=1.0.9
  Downloading Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.7/2.7 MB 196.0 MB/s eta 0:00:00
Requirement already satisfied: einops in /usr/lib/python3/dist-packages (from flash-attn==0.2.2->-r llm/requirements.txt (line 6)) (0.6.0)
Collecting cmake
  Downloading cmake-3.25.0-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 109.2 MB/s eta 0:00:00
Collecting filelock
  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)
Collecting regex!=2019.12.17
  Downloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 770.5/770.5 kB 154.3 MB/s eta 0:00:00
Collecting tokenizers!=0.11.3,<0.14,>=0.11.1
  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 7.6/7.6 MB 205.8 MB/s eta 0:00:00
Collecting huggingface-hub<1.0,>=0.10.0
  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.3/190.3 kB 100.5 MB/s eta 0:00:00
Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.24.0->-r llm/requirements.txt (line 9)) (23.0)
Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.24.0->-r llm/requirements.txt (line 9)) (6.0)
Collecting fsspec[http]>=2021.11.1
  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 143.0/143.0 kB 81.7 MB/s eta 0:00:00
Collecting responses<0.19
  Downloading responses-0.18.0-py3-none-any.whl (38 kB)
Collecting dill<0.3.7
  Downloading dill-0.3.6-py3-none-any.whl (110 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 110.5/110.5 kB 70.2 MB/s eta 0:00:00
Collecting aiohttp
  Downloading aiohttp-3.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 182.5 MB/s eta 0:00:00
Collecting pyarrow>=6.0.0
  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 35.9/35.9 MB 101.0 MB/s eta 0:00:00
Collecting multiprocess
  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 134.3/134.3 kB 77.6 MB/s eta 0:00:00
Collecting antlr4-python3-runtime==4.9.*
  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 117.0/117.0 kB 70.4 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting GitPython>=1.0.0
  Downloading GitPython-3.1.30-py3-none-any.whl (184 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 184.0/184.0 kB 93.4 MB/s eta 0:00:00
Collecting docker-pycreds>=0.4.0
  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)
Collecting sentry-sdk>=1.0.0
  Downloading sentry_sdk-1.14.0-py2.py3-none-any.whl (178 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 178.9/178.9 kB 101.4 MB/s eta 0:00:00
Collecting shortuuid>=0.5.0
  Downloading shortuuid-1.0.11-py3-none-any.whl (10 kB)
Collecting pathtools
  Downloading pathtools-0.1.2.tar.gz (11 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting Click!=8.0.0,>=7.0
  Downloading click-8.1.3-py3-none-any.whl (96 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 kB 60.9 MB/s eta 0:00:00
Collecting setproctitle
  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)
Collecting psutil>=5.0.0
  Downloading psutil-5.9.4-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (280 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 280.2/280.2 kB 114.9 MB/s eta 0:00:00
Collecting protobuf!=4.0.*,!=4.21.0,<5,>=3.12.0
  Downloading protobuf-4.21.12-cp37-abi3-manylinux2014_x86_64.whl (409 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 409.8/409.8 kB 131.7 MB/s eta 0:00:00
Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb==0.13.4->-r llm/requirements.txt (line 12)) (65.6.3)
Collecting promise<3,>=2.0
  Downloading promise-2.3.tar.gz (19 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Requirement already satisfied: six>=1.13.0 in /usr/lib/python3/dist-packages (from wandb==0.13.4->-r llm/requirements.txt (line 12)) (1.14.0)
Collecting packaging>=20.0
  Downloading packaging-22.0-py3-none-any.whl (42 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 42.6/42.6 kB 28.5 MB/s eta 0:00:00
Collecting torchmetrics<0.10.0,>=0.7.0
  Downloading torchmetrics-0.9.3-py3-none-any.whl (419 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 419.6/419.6 kB 139.9 MB/s eta 0:00:00
Collecting importlib-metadata<6,>=5.0.0
  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)
Collecting requests
  Downloading requests-2.28.2-py3-none-any.whl (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 43.8 MB/s eta 0:00:00
Collecting coolname<3,>=1.1.0
  Downloading coolname-2.2.0-py2.py3-none-any.whl (37 kB)
Collecting tabulate==0.9.0
  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)
Collecting torch-optimizer<0.4,>=0.3.0
  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.9/61.9 kB 42.7 MB/s eta 0:00:00
Collecting py-cpuinfo<10,>=8.0.0
  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)
Collecting numpy
  Downloading numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.8/16.8 MB 165.8 MB/s eta 0:00:00
Collecting sqlitedict
  Downloading sqlitedict-2.1.0.tar.gz (21 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting sacrebleu==1.5.0
  Downloading sacrebleu-1.5.0-py3-none-any.whl (65 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.6/65.6 kB 43.0 MB/s eta 0:00:00
Collecting lm_dataformat==0.0.20
  Downloading lm_dataformat-0.0.20-py3-none-any.whl (5.8 kB)
Collecting zstandard==0.15.2
  Downloading zstandard-0.15.2.tar.gz (1.0 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 175.5 MB/s eta 0:00:00
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting mock==4.0.3
  Downloading mock-4.0.3-py3-none-any.whl (28 kB)
Collecting jieba==0.42.1
  Downloading jieba-0.42.1.tar.gz (19.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 19.2/19.2 MB 107.8 MB/s eta 0:00:00
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting jsonlines
  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)
Collecting numexpr
  Downloading numexpr-2.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 381.4/381.4 kB 118.0 MB/s eta 0:00:00
Collecting openai>=0.6.4
  Downloading openai-0.26.3.tar.gz (55 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 55.5/55.5 kB 39.1 MB/s eta 0:00:00
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting pybind11>=2.6.2
  Downloading pybind11-2.10.3-py3-none-any.whl (222 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 222.4/222.4 kB 63.5 MB/s eta 0:00:00
Collecting pycountry
  Downloading pycountry-22.3.5.tar.gz (10.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.1/10.1 MB 201.2 MB/s eta 0:00:00
  Installing build dependencies: started
  Installing build dependencies: finished with status 'done'
  Getting requirements to build wheel: started
  Getting requirements to build wheel: finished with status 'done'
  Preparing metadata (pyproject.toml): started
  Preparing metadata (pyproject.toml): finished with status 'done'
Collecting pytablewriter
  Downloading pytablewriter-0.64.2-py3-none-any.whl (106 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.6/106.6 kB 68.7 MB/s eta 0:00:00
Collecting rouge-score>=0.0.4
  Downloading rouge_score-0.1.2.tar.gz (17 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting scikit-learn>=0.24.1
  Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.6/9.6 MB 203.2 MB/s eta 0:00:00
Collecting tqdm-multiprocess
  Downloading tqdm_multiprocess-0.0.11-py3-none-any.whl (9.8 kB)
Collecting ujson
  Downloading ujson-5.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (52 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 52.8/52.8 kB 38.3 MB/s eta 0:00:00
Collecting portalocker
  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)
Requirement already satisfied: tomli>=1.0.0 in /usr/lib/python3/dist-packages (from pytest->-r llm/requirements.txt (line 14)) (2.0.1)
Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/lib/python3/dist-packages (from pytest->-r llm/requirements.txt (line 14)) (1.1.0)
Requirement already satisfied: iniconfig in /usr/lib/python3/dist-packages (from pytest->-r llm/requirements.txt (line 14)) (2.0.0)
Requirement already satisfied: attrs>=19.2.0 in /usr/lib/python3/dist-packages (from pytest->-r llm/requirements.txt (line 14)) (22.2.0)
Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/lib/python3/dist-packages (from pytest->-r llm/requirements.txt (line 14)) (1.0.0)
Collecting python-dateutil>=2.8.1
  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 247.7/247.7 kB 102.9 MB/s eta 0:00:00
Collecting pytz>=2020.1
  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 499.4/499.4 kB 151.4 MB/s eta 0:00:00
Collecting botocore<1.30.0,>=1.29.57
  Downloading botocore-1.29.57-py3-none-any.whl (10.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.4/10.4 MB 204.8 MB/s eta 0:00:00
Collecting jmespath<2.0.0,>=0.7.1
  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)
Collecting s3transfer<0.7.0,>=0.6.0
  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 79.6/79.6 kB 55.9 MB/s eta 0:00:00
Collecting charset-normalizer<3.0,>=2.0
  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)
Collecting multidict<7.0,>=4.5
  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 114.5/114.5 kB 69.7 MB/s eta 0:00:00
Collecting aiosignal>=1.1.2
  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)
Collecting async-timeout<5.0,>=4.0.0a3
  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)
Collecting yarl<2.0,>=1.0
  Downloading yarl-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 264.0/264.0 kB 117.5 MB/s eta 0:00:00
Collecting frozenlist>=1.1.1
  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 149.6/149.6 kB 81.2 MB/s eta 0:00:00
Collecting gitdb<5,>=4.0.1
  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 42.6 MB/s eta 0:00:00
Collecting zipp>=0.5
  Downloading zipp-3.11.0-py3-none-any.whl (6.6 kB)
Collecting kiwisolver>=1.0.1
  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 201.5 MB/s eta 0:00:00
Collecting cycler>=0.10
  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)
Collecting fonttools>=4.22.0
  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 965.4/965.4 kB 187.2 MB/s eta 0:00:00
Collecting pyparsing>=2.2.1
  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 98.3/98.3 kB 65.6 MB/s eta 0:00:00
Collecting contourpy>=1.0.1
  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 300.3/300.3 kB 125.5 MB/s eta 0:00:00
Collecting circuitbreaker<2.0.0,>=1.3.1
  Downloading circuitbreaker-1.4.0.tar.gz (9.7 kB)
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Collecting cryptography<39.0.0,>=3.2.1
  Downloading cryptography-38.0.4-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 205.4 MB/s eta 0:00:00
Collecting pyOpenSSL<=22.1.0,>=17.5.0
  Downloading pyOpenSSL-22.1.0-py3-none-any.whl (57 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.0/57.0 kB 38.5 MB/s eta 0:00:00
Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from oci<3,>=2.88->mosaicml-streaming==0.2.2->-r llm/requirements.txt (line 5)) (2019.11.28)
Collecting bcrypt>=3.1.3
  Downloading bcrypt-4.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (593 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 593.7/593.7 kB 161.1 MB/s eta 0:00:00
Collecting pynacl>=1.0.1
  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 856.7/856.7 kB 171.0 MB/s eta 0:00:00
Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchvision==0.14.0->-r llm/requirements.txt (line 1)) (2.8)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->torchvision==0.14.0->-r llm/requirements.txt (line 1)) (1.25.8)
Collecting urllib3<1.27,>=1.21.1
  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 140.6/140.6 kB 79.9 MB/s eta 0:00:00
Collecting absl-py
  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.5/126.5 kB 76.5 MB/s eta 0:00:00
Collecting nltk
  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.5/1.5 MB 192.3 MB/s eta 0:00:00
Collecting joblib>=1.1.1
  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.0/298.0 kB 125.2 MB/s eta 0:00:00
Collecting scipy>=1.3.2
  Downloading scipy-1.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.4/34.4 MB 108.5 MB/s eta 0:00:00
Collecting threadpoolctl>=2.0.0
  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)
Collecting pytorch-ranger>=0.1.1
  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)
Collecting pathvalidate<3,>=2.3.0
  Downloading pathvalidate-2.5.2-py3-none-any.whl (20 kB)
Collecting DataProperty<2,>=0.55.0
  Downloading DataProperty-0.55.0-py3-none-any.whl (26 kB)
Collecting tabledata<2,>=1.3.0
  Downloading tabledata-1.3.0-py3-none-any.whl (11 kB)
Collecting tcolorpy<1,>=0.0.5
  Downloading tcolorpy-0.1.2-py3-none-any.whl (7.9 kB)
Collecting mbstrdecoder<2,>=1.0.0
  Downloading mbstrdecoder-1.1.1-py3-none-any.whl (7.7 kB)
Collecting typepy[datetime]<2,>=1.2.0
  Downloading typepy-1.3.0-py3-none-any.whl (31 kB)
Collecting colorama
  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)
Collecting cffi>=1.12
  Downloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 441.8/441.8 kB 138.8 MB/s eta 0:00:00
Collecting smmap<6,>=3.0.1
  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)
Requirement already satisfied: chardet<6,>=3.0.4 in /usr/lib/python3/dist-packages (from mbstrdecoder<2,>=1.0.0->pytablewriter->lm-eval@ git+https://github.com/bmosaicml/lm-evaluation-harness-composergpt-integration.git@master->-r llm/requirements.txt (line 13)) (3.0.4)
Collecting pycparser
  Downloading pycparser-2.21-py2.py3-none-any.whl (118 kB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 118.7/118.7 kB 55.1 MB/s eta 0:00:00
Building wheels for collected packages: antlr4-python3-runtime, mosaicml, lm-eval, jieba, zstandard, openai, promise, rouge-score, pathtools, pycountry, sqlitedict, circuitbreaker
  Building wheel for antlr4-python3-runtime (setup.py): started
  Building wheel for antlr4-python3-runtime (setup.py): finished with status 'done'
  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=6a7867ba48c22c8574c3b85cf5429310aad25a8605d59640bb79d6380b58a2e0
  Stored in directory: /root/.cache/pip/wheels/48/6a/c2/acb58c7afdf57e4cddf5e1513f5a2d62aa8e98f82a00c76d7c
  Building wheel for mosaicml (pyproject.toml): started
  Building wheel for mosaicml (pyproject.toml): finished with status 'done'
  Created wheel for mosaicml: filename=mosaicml-0.12.0-py3-none-any.whl size=524265 sha256=178d5a3323fec6161ec6691bd28eaf7eee0c22369762839d5b48c2f449479e11
  Stored in directory: /tmp/pip-ephem-wheel-cache-6kqgozx7/wheels/54/81/0f/08c76ba1fc1c002f8b8f2e3c51176d0cfc55b15e160efe9773
  Building wheel for lm-eval (setup.py): started
  Building wheel for lm-eval (setup.py): finished with status 'done'
  Created wheel for lm-eval: filename=lm_eval-0.3.0-py3-none-any.whl size=183056 sha256=987c92af40ed9b4d68b7915ea3ccf5d63e47ffb4392751d0346f77397a83cfc2
  Stored in directory: /tmp/pip-ephem-wheel-cache-6kqgozx7/wheels/41/6d/2d/86220a53f6564458d2f20a5059e7e445a8b6c967e9470b1df5
  Building wheel for jieba (setup.py): started
  Building wheel for jieba (setup.py): finished with status 'done'
  Created wheel for jieba: filename=jieba-0.42.1-py3-none-any.whl size=19314458 sha256=94dd659d20248cf25d10d32d908628cf4512a6cf14c35bf67c4d93d328cd0d5e
  Stored in directory: /root/.cache/pip/wheels/fe/aa/79/217233b7c4512e033cb980e378a27a73ff3c5359d2a48740f5
  Building wheel for zstandard (pyproject.toml): started
  Building wheel for zstandard (pyproject.toml): finished with status 'done'
  Created wheel for zstandard: filename=zstandard-0.15.2-cp310-cp310-linux_x86_64.whl size=2141453 sha256=5075fb9f50494abf019e61d48f7937392cc1d1bbf9029da073bac827501bfa72
  Stored in directory: /root/.cache/pip/wheels/c7/b5/90/e59f5a6a75de392b795cba46b3ddc62e455222dc132258a6c4
  Building wheel for openai (pyproject.toml): started
  Building wheel for openai (pyproject.toml): finished with status 'done'
  Created wheel for openai: filename=openai-0.26.3-py3-none-any.whl size=67466 sha256=cdb0f3a554ac15d363c0a2edfeb924049f9a3739f8496200b2e46f03f57f661b
  Stored in directory: /root/.cache/pip/wheels/56/01/1f/03d0db743c85ec79741d06ca11808e50a96d7f4fcb4c21b9f7
  Building wheel for promise (setup.py): started
  Building wheel for promise (setup.py): finished with status 'done'
  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21486 sha256=b346a92a68416a0d10adf5fa64c5f4be07d39be9f402801a26b4035052e901ed
  Stored in directory: /root/.cache/pip/wheels/76/40/54/417a4d64a01b61b247658d83597e1dc83c3de01fc0cef44972
  Building wheel for rouge-score (setup.py): started
  Building wheel for rouge-score (setup.py): finished with status 'done'
  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=111ed44259123bf3d9af5c228059d4a9283993013eb09da6e8f4d6639c0af3b2
  Stored in directory: /root/.cache/pip/wheels/3e/94/5c/7ff8a51c53c1bbc8df4cac58aa4990ffbc6fa203e9f0808fdd
  Building wheel for pathtools (setup.py): started
  Building wheel for pathtools (setup.py): finished with status 'done'
  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=5a940e22df38ded2adfdb04fdcec68770e7dbf598cd50b330c37967136916ae6
  Stored in directory: /root/.cache/pip/wheels/44/1b/54/249c94316d4e1030e2d0683fba1d8ea06197de866f5a4de738
  Building wheel for pycountry (pyproject.toml): started
  Building wheel for pycountry (pyproject.toml): finished with status 'done'
  Created wheel for pycountry: filename=pycountry-22.3.5-py2.py3-none-any.whl size=10681832 sha256=3c4632f54ff0da22ec0b0a36aaab85a0abf9442388bbbd056083b7a32d346cb8
  Stored in directory: /root/.cache/pip/wheels/62/4a/9c/7a46699df9efb845aa116fae5e52d8690fc442fef6d32213f7
  Building wheel for sqlitedict (setup.py): started
  Building wheel for sqlitedict (setup.py): finished with status 'done'
  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=e21db82dcab8d84daf02dd0e98986d80354e600106c6983b97abd0d00dc4feaf
  Stored in directory: /root/.cache/pip/wheels/ea/40/8f/1e0ab348b0b97c9d97c5da29090d2a3d7d1d2a60e4e5edd357
  Building wheel for circuitbreaker (setup.py): started
  Building wheel for circuitbreaker (setup.py): finished with status 'done'
  Created wheel for circuitbreaker: filename=circuitbreaker-1.4.0-py3-none-any.whl size=7519 sha256=2dd4a7d70ce9298707821f5b535df8445008213b23554acf851575c9eeefa66c
  Stored in directory: /root/.cache/pip/wheels/21/8c/34/be8b08101a63ca22d5a9ba0b4a39b7ed9464c27566076aa7d4
Successfully built antlr4-python3-runtime mosaicml lm-eval jieba zstandard openai promise rouge-score pathtools pycountry sqlitedict circuitbreaker
Installing collected packages: zstd, tokenizers, sqlitedict, pytz, python-snappy, py-cpuinfo, pathtools, jieba, coolname, cmake, circuitbreaker, Brotli, antlr4-python3-runtime, zstandard, zipp, xxhash, urllib3, ujson, threadpoolctl, tcolorpy, tabulate, smmap, shortuuid, setproctitle, regex, python-dateutil, pyparsing, pycparser, pycountry, pybind11, psutil, protobuf, promise, portalocker, pathvalidate, packaging, omegaconf, numpy, multidict, mock, mbstrdecoder, kiwisolver, jsonlines, joblib, jmespath, fsspec, frozenlist, fonttools, filelock, docker-pycreds, dill, cycler, colorama, Click, charset-normalizer, bcrypt, async-timeout, absl-py, yarl, typepy, triton, tqdm-multiprocess, torchmetrics, sentry-sdk, scipy, sacrebleu, requests, pytorch-ranger, pyarrow, pandas, numexpr, nltk, multiprocess, lm_dataformat, importlib-metadata, gitdb, contourpy, cffi, botocore, aiosignal, torch-optimizer, scikit-learn, s3transfer, rouge-score, responses, pynacl, matplotlib, huggingface-hub, GitPython, cryptography, aiohttp, wandb, transformers, pyOpenSSL, paramiko, openai, mosaicml, DataProperty, boto3, tabledata, oci, datasets, pytablewriter, mosaicml-streaming, lm-eval
  Attempting uninstall: urllib3
    Found existing installation: urllib3 1.25.8
    Uninstalling urllib3-1.25.8:
      Successfully uninstalled urllib3-1.25.8
  Attempting uninstall: packaging
    Found existing installation: packaging 23.0
    Uninstalling packaging-23.0:
      Successfully uninstalled packaging-23.0
  Attempting uninstall: numpy
    Found existing installation: numpy 1.24.1
    Uninstalling numpy-1.24.1:
      Successfully uninstalled numpy-1.24.1
  Attempting uninstall: requests
    Found existing installation: requests 2.22.0
    Uninstalling requests-2.22.0:
      Successfully uninstalled requests-2.22.0
Successfully installed Brotli-1.0.9 Click-8.1.3 DataProperty-0.55.0 GitPython-3.1.30 absl-py-1.4.0 aiohttp-3.8.3 aiosignal-1.3.1 antlr4-python3-runtime-4.9.3 async-timeout-4.0.2 bcrypt-4.0.1 boto3-1.26.57 botocore-1.29.57 cffi-1.15.1 charset-normalizer-2.1.1 circuitbreaker-1.4.0 cmake-3.25.0 colorama-0.4.6 contourpy-1.0.7 coolname-2.2.0 cryptography-38.0.4 cycler-0.11.0 datasets-2.7.1 dill-0.3.6 docker-pycreds-0.4.0 filelock-3.9.0 fonttools-4.38.0 frozenlist-1.3.3 fsspec-2023.1.0 gitdb-4.0.10 huggingface-hub-0.12.0 importlib-metadata-5.2.0 jieba-0.42.1 jmespath-1.0.1 joblib-1.2.0 jsonlines-3.1.0 kiwisolver-1.4.4 lm-eval-0.3.0 lm_dataformat-0.0.20 matplotlib-3.6.3 mbstrdecoder-1.1.1 mock-4.0.3 mosaicml-0.12.0 mosaicml-streaming-0.2.2 multidict-6.0.4 multiprocess-0.70.14 nltk-3.8.1 numexpr-2.8.4 numpy-1.22.4 oci-2.90.3 omegaconf-2.2.3 openai-0.26.3 packaging-23.0 pandas-1.5.3 paramiko-2.12.0 pathtools-0.1.2 pathvalidate-2.5.2 portalocker-2.7.0 promise-2.3 protobuf-4.21.12 psutil-5.9.4 py-cpuinfo-9.0.0 pyOpenSSL-22.1.0 pyarrow-10.0.1 pybind11-2.10.3 pycountry-22.3.5 pycparser-2.21 pynacl-1.5.0 pyparsing-3.0.9 pytablewriter-0.64.2 python-dateutil-2.8.2 python-snappy-0.6.1 pytorch-ranger-0.1.1 pytz-2022.7.1 regex-2022.10.31 requests-2.28.2 responses-0.18.0 rouge-score-0.1.2 s3transfer-0.6.0 sacrebleu-1.5.0 scikit-learn-1.2.1 scipy-1.10.0 sentry-sdk-1.14.0 setproctitle-1.3.2 shortuuid-1.0.11 smmap-5.0.0 sqlitedict-2.1.0 tabledata-1.3.0 tabulate-0.9.0 tcolorpy-0.1.2 threadpoolctl-3.1.0 tokenizers-0.13.2 torch-optimizer-0.3.0 torchmetrics-0.9.3 tqdm-multiprocess-0.0.11 transformers-4.24.0 triton-2.0.0.dev20221103 typepy-1.3.0 ujson-5.7.0 urllib3-1.26.14 wandb-0.13.4 xxhash-3.2.0 yarl-1.8.2 zipp-3.11.0 zstandard-0.15.2 zstd-1.5.2.6
WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv
/
Building MosaicGPT w/ config: 
data_local: ./my-copy-c4
data_remote: null
tokenizer_name: gpt2
max_seq_len: 2048
global_seed: 17
run_name: gpt-70b
model:
  name: mosaic_gpt
  device: meta
  tokenizer_name: gpt2
  d_model: 8192
  n_heads: 64
  n_layers: 250
  mlp_ratio: 4
  max_seq_len: 2048
  vocab_size: 50257
  init_std: 0.02
  attn_pdrop: 0.0
  resid_pdrop: 0.0
  emb_pdrop: 0.0
  attn_impl: flash
tokenizer:
  type: hftokenizer
  args:
    tokenizer_name: gpt2
    max_seq_len: 2048
train_loader:
  name: text
  dataset:
    local: ./my-copy-c4
    remote: null
    split: train
    shuffle: true
    tokenizer_name: gpt2
    max_seq_len: 2048
    group_method: concat
    shuffle_seed: 17
  drop_last: true
  num_workers: 8
eval_loader:
  name: text
  dataset:
    local: ./my-copy-c4
    remote: null
    split: val
    shuffle: false
    tokenizer_name: gpt2
    max_seq_len: 2048
    group_method: truncate
    shuffle_seed: 17
  drop_last: false
  num_workers: 8
scheduler:
  name: cosine_with_warmup
  t_warmup: 100ba
  alpha_f: 0.1
optimizer:
  name: decoupled_adamw
  lr: 8.0e-05
  betas:
  - 0.9
  - 0.95
  eps: 1.0e-08
  weight_decay: 0.0
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
max_duration: 333800ba
eval_interval: 10000ba
eval_subset_num_batches: -1
global_train_batch_size: 2048
seed: 17
device_eval_batch_size: 4
device_train_microbatch_size: 4
precision: amp_bf16
fsdp_config:
  sharding_strategy: FULL_SHARD
  min_params: 600000000.0
  mixed_precision: PURE
  activation_checkpointing: true
  activation_cpu_offload: false
  verbose: true
progress_bar: false
log_to_console: true
console_log_interval: 1ba
callbacks:
  speed_monitor:
    window_size: 10
  lr_monitor: {}
  memory_monitor: {}

Initializing model...
n_params=2.02e+11
Using pad_token, but it is not set yet.
Downloading lambada/lambada_test.json:   0%|          | 0.00/1.90M [00:00<?, ?iB/s]Downloading lambada/lambada_test.json:  14%|█▍        | 262k/1.90M [00:00<00:00, 2.44MiB/s]Downloading lambada/lambada_test.json:   3%|▎         | 64.3k/1.90M [00:00<00:03, 585kiB/s]
Using custom data configuration default-6aae1ca5f3efbe3b
Found cached dataset json (/root/.cache/huggingface/datasets/json/default-6aae1ca5f3efbe3b/0.0.0/e6070c77f18f01a5ad4551a8b7edfba20b8438b7cad4d94e6ad9378022ce4aab)
  0%|          | 0/5151 [00:00<?, ?ex/s] 60%|█████▉    | 3086/5151 [00:00<00:00, 30857.53ex/s]100%|██████████| 5151/5151 [00:00<00:00, 37600.48ex/s]
  0%|          | 0/5151 [00:00<?, ?it/s]  5%|▌         | 277/5151 [00:00<00:01, 2767.10it/s] 12%|█▏        | 634/5151 [00:00<00:01, 3236.00it/s] 20%|█▉        | 1009/5151 [00:00<00:01, 3467.99it/s] 27%|██▋       | 1392/5151 [00:00<00:01, 3607.79it/s] 35%|███▍      | 1784/5151 [00:00<00:00, 3718.33it/s] 42%|████▏     | 2156/5151 [00:00<00:01, 2678.00it/s] 50%|████▉     | 2558/5151 [00:00<00:00, 3025.30it/s] 58%|█████▊    | 2972/5151 [00:00<00:00, 3325.50it/s] 66%|██████▌   | 3395/5151 [00:01<00:00, 3576.71it/s] 74%|███████▍  | 3821/5151 [00:01<00:00, 3769.03it/s] 83%|████████▎ | 4253/5151 [00:01<00:00, 3927.32it/s] 91%|█████████ | 4691/5151 [00:01<00:00, 4058.76it/s]100%|█████████▉| 5150/5151 [00:01<00:00, 4213.13it/s]100%|██████████| 5151/5151 [00:01<00:00, 3637.03it/s]
/usr/lib/python3/dist-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
FSDP: Wrapped Model:
ComposerMosaicGPT(
  (model): FullyShardedDataParallel(
    (_fsdp_wrapped_module): FlattenParamsWrapper(
      (_fpw_module): MosaicGPT(
        (transformer): ModuleDict(
          (wte): Embedding(50257, 8192)
          (wpe): Embedding(2048, 8192)
          (emb_drop): Dropout(p=0.0, inplace=False)
          (blocks): ModuleList(
            (0): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (1): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (2): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (3): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (4): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (5): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (6): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (7): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (8): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (9): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (10): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (11): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (12): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (13): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (14): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (15): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (16): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (17): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (18): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (19): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (20): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (21): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (22): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (23): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (24): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (25): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (26): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (27): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (28): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (29): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (30): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (31): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (32): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (33): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (34): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (35): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (36): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (37): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (38): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (39): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (40): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (41): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (42): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (43): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (44): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (45): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (46): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (47): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (48): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (49): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (50): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (51): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (52): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (53): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (54): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (55): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (56): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (57): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (58): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (59): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (60): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (61): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (62): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (63): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (64): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (65): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (66): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (67): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (68): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (69): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (70): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (71): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (72): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (73): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (74): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (75): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (76): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (77): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (78): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (79): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (80): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (81): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (82): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (83): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (84): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (85): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (86): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (87): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (88): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (89): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (90): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (91): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (92): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (93): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (94): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (95): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (96): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (97): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (98): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (99): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (100): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (101): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (102): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (103): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (104): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (105): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (106): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (107): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (108): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (109): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (110): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (111): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (112): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (113): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (114): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (115): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (116): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (117): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (118): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (119): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (120): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (121): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (122): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (123): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (124): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (125): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (126): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (127): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (128): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (129): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (130): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (131): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (132): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (133): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (134): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (135): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (136): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (137): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (138): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (139): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (140): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (141): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (142): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (143): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (144): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (145): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (146): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (147): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (148): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (149): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (150): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (151): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (152): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (153): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (154): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (155): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (156): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (157): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (158): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (159): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (160): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (161): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (162): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (163): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (164): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (165): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (166): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (167): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (168): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (169): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (170): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (171): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (172): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (173): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (174): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (175): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (176): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (177): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (178): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (179): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (180): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (181): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (182): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (183): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (184): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (185): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (186): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (187): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (188): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (189): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (190): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (191): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (192): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (193): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (194): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (195): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (196): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (197): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (198): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (199): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (200): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (201): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (202): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (203): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (204): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (205): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (206): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (207): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (208): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (209): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (210): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (211): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (212): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (213): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (214): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (215): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (216): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (217): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (218): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (219): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (220): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (221): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (222): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (223): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (224): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (225): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (226): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (227): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (228): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (229): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (230): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (231): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (232): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (233): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (234): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (235): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (236): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (237): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (238): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (239): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (240): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (241): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (242): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (243): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (244): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (245): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (246): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (247): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (248): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
            (249): FullyShardedDataParallel(
              (_fsdp_wrapped_module): FlattenParamsWrapper(
                (_fpw_module): CheckpointWrapper(
                  (_checkpoint_wrapped_module): GPTBlock(
                    (ln_1): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (causal_attn): FlashCausalAttention(
                      (mhsa): FlashMHA(
                        (Wqkv): Linear(in_features=8192, out_features=24576, bias=True)
                        (inner_attn): FlashAttention()
                        (out_proj): Linear(in_features=8192, out_features=8192, bias=True)
                      )
                    )
                    (ln_2): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
                    (mlp): GPTMLP(
                      (mlp_up): Linear(in_features=8192, out_features=32768, bias=True)
                      (mlp_act): GELU(approximate='none')
                      (mlp_down): Linear(in_features=32768, out_features=8192, bias=True)
                    )
                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)
                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)
                  )
                )
              )
            )
          )
          (ln_f): LayerNorm((8192,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
  )
)
FSDP: Using sharding_strategy=ShardingStrategy.FULL_SHARD
FSDP: Using cpu_offload=None
FSDP: Using mixed_precision=MixedPrecision(param_dtype=torch.float16, reduce_dtype=torch.float16, buffer_dtype=torch.float16, keep_low_precision_grads=False)
FSDP: Using backward_prefetch=BackwardPrefetch.BACKWARD_POST
FSDP: Using min_params=600000000
FSDP: Using activation_checkpointing=True
FSDP: Using activation_cpu_offload=False
FSDP: Using sync_module_states=False
FSDP: Using forward_prefetch=False
FSDP: Using limit_all_gathers=False
/usr/lib/python3/dist-packages/composer/trainer/trainer.py:988: UserWarning: No optimizer was specified. Defaulting to DecoupledSGDW(lr=0.1)
  warnings.warn(('No optimizer was specified. Defaulting to '
/usr/lib/python3/dist-packages/composer/trainer/trainer.py:1085: UserWarning: Setting both `progress_bar` and `log_to_console` both to True is not recommended and willlead to duplicate logs and weird formatting issues. Please set one of them to False for a better logging experience.
  warnings.warn(
                                                                                                                       ******************************
Config:
num_gpus_per_node: 8
num_nodes: 8
rank_zero_seed: 849760561

******************************

lambada_0-shot Epoch   0:    0%|                         | 0/41 [00:00<?, ?ba/s]                                        [A******************************
Config:
num_gpus_per_node: 8
num_nodes: 8
rank_zero_seed: 849760561

******************************
/usr/lib/python3/dist-packages/torch/distributed/distributed_c10d.py:2387: UserWarning: torch.distributed._all_gather_base is a private function and will be deprecated. Please use torch.distributed.all_gather_into_tensor instead.
  warnings.warn(
/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")

lambada_0-shot Epoch   0:    2%|▌                        | 1/41 [00:09<06:33,  9.85s/ba]                                [A
lambada_0-shot Epoch   0:    5%|█▏                       | 2/41 [00:18<05:46,  8.89s/ba]                                [A
lambada_0-shot Epoch   0:    7%|█▊                       | 3/41 [00:26<05:26,  8.60s/ba]                                [A
lambada_0-shot Epoch   0:   10%|██▍                      | 4/41 [00:34<05:13,  8.48s/ba]                                [A
lambada_0-shot Epoch   0:   12%|███                      | 5/41 [00:42<05:02,  8.41s/ba]                                [A
lambada_0-shot Epoch   0:   15%|███▋                     | 6/41 [00:51<04:53,  8.38s/ba]                                [A
lambada_0-shot Epoch   0:   17%|████▎                    | 7/41 [00:59<04:44,  8.36s/ba]                                [A
lambada_0-shot Epoch   0:   20%|████▉                    | 8/41 [01:07<04:35,  8.35s/ba]                                [A
lambada_0-shot Epoch   0:   22%|█████▍                   | 9/41 [01:16<04:26,  8.34s/ba]                                [A
lambada_0-shot Epoch   0:   24%|██████                   | 10/41 [01:24<04:18,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   27%|██████▋                  | 11/41 [01:32<04:10,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   29%|███████▎                 | 12/41 [01:41<04:01,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   32%|███████▉                 | 13/41 [01:49<03:53,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   34%|████████▌                | 14/41 [01:57<03:45,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   37%|█████████▏               | 15/41 [02:06<03:36,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   39%|█████████▊               | 16/41 [02:14<03:28,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   41%|██████████▎              | 17/41 [02:22<03:20,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   44%|██████████▉              | 18/41 [02:31<03:11,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   46%|███████████▌             | 19/41 [02:39<03:03,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   49%|████████████▏            | 20/41 [02:47<02:55,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   51%|████████████▊            | 21/41 [02:56<02:46,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   54%|█████████████▍           | 22/41 [03:04<02:38,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   56%|██████████████           | 23/41 [03:12<02:30,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   59%|██████████████▋          | 24/41 [03:21<02:21,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   61%|███████████████▏         | 25/41 [03:29<02:13,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   63%|███████████████▊         | 26/41 [03:37<02:05,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   66%|████████████████▍        | 27/41 [03:46<01:56,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   68%|█████████████████        | 28/41 [03:54<01:48,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   71%|█████████████████▋       | 29/41 [04:02<01:40,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   73%|██████████████████▎      | 30/41 [04:11<01:31,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   76%|██████████████████▉      | 31/41 [04:19<01:23,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   78%|███████████████████▌     | 32/41 [04:28<01:15,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   80%|████████████████████     | 33/41 [04:36<01:06,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   83%|████████████████████▋    | 34/41 [04:44<00:58,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   85%|█████████████████████▎   | 35/41 [04:53<00:50,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   88%|█████████████████████▉   | 36/41 [05:01<00:41,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   90%|██████████████████████▌  | 37/41 [05:09<00:33,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   93%|███████████████████████▏ | 38/41 [05:18<00:25,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   95%|███████████████████████▊ | 39/41 [05:26<00:16,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:   98%|████████████████████████▍| 40/41 [05:34<00:08,  8.34s/ba]                               [A
lambada_0-shot Epoch   0:  100%|█████████████████████████| 41/41 [05:40<00:00,  7.46s/ba]                               [A
lambada_0-shot Epoch   0:  100%|█████████████████████████| 41/41 [05:40<00:00,  7.46s/ba]                               [A
lambada_0-shot Epoch   0:  100%|█████████████████████████| 41/41 [05:40<00:00,  7.46s/ba]                               [A
lambada_0-shot Epoch   0:  100%|█████████████████████████| 41/41 [05:40<00:00,  8.30s/ba]                               
:
	 Eval InContextLearningLMAccuracy: 0.0000
Ran eval in: 340.16874504089355 seconds
metrics/lambada_0-shot/InContextLearningLMAccuracy: 0.0

                                                                                                                       
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
