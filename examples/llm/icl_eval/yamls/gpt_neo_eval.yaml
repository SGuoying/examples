# Tokenizer
tokenizer:
    name:  EleutherAI/gpt-neo-2.7B
    kwargs:
      model_max_length: 2048

model:
  name: hf_causal_lm
  pretrained_model_name_or_path:  EleutherAI/gpt-neo-2.7B
  device: cpu
  pretrained: true

load_path: # Add your (optional) Composer checkpoint path here!

# FSDP config for model sharding
# fsdp_config:
#   sharding_strategy: FULL_SHARD
#   mixed_precision: PURE

icl_tasks:
  -
    label: lambada
    dataset_uri: oci://mosaicml-internal-datasets/icl/original/lambada_openai.jsonl  # ADD YOUR OWN DATASET URI
    num_fewshot:
    - 0
    batch_size: 1
    icl_task_type: language_modeling
    metric_names:
    - InContextLearningLMAccuracy
    - InContextLearningLMExpectedCalibrationError
    prompt_string: '' # this goes at the beginning of each input
    example_delimiter: '\n' # this goes between fewshot examples
    continuation_delimiter: ' ' # this separates contexts from continuations
  -
    label: clean_lambada
    dataset_uri: oci://mosaicml-internal-datasets/icl/clean/clean_lambada_openai.jsonl  # ADD YOUR OWN DATASET URI
    num_fewshot:
    - 0
    batch_size: 1
    icl_task_type: language_modeling
    metric_names:
    - InContextLearningLMAccuracy
    - InContextLearningLMExpectedCalibrationError
    prompt_string: '' # this goes at the beginning of each input
    example_delimiter: '\n' # this goes between fewshot examples
    continuation_delimiter: ' ' # this separates contexts from continuations
  