integrations:
- integration_type: git_repo
  git_repo: mosaicml/examples
  git_branch: testing/test_opt  # use your branch
  # git_commit: # OR use your commit hash
  pip_install: -e .[llm]
  ssh_clone: false

command: |
  cd examples/examples/llm/icl_eval
  composer evaluate_model.py /mnt/config/parameters.yaml

image: mosaicml/pytorch:1.13.1_cu117-python3.10-ubuntu20.04
optimization_level: 0

run_name: mosaic-gpt-1b-eval
gpu_num: 8
gpu_type: a100_80gb
cluster: r1z1 # replace with your cluster here!

# The below is injected as a YAML file: /mnt/config/parameters.yaml
parameters:
  # Tokenizer
  tokenizer:
    type: hftokenizer
    args:
      tokenizer_name: facebook/opt-1.3b
      max_seq_len: 2048

  model:
    name: hf_causal_lm
    pretrained_model_name_or_path: facebook/opt-1.3b
    device: cpu
    pretrained: true

  # load_path: s3://mosaicml-internal-checkpoints-shared/nlp/mosaicgpt/mosaicgpt-1b-TaT8te/checkpoints/ep0-ba24000-rank0.pt

  # FSDP config for model sharding
  # fsdp_config:
  #   sharding_strategy: FULL_SHARD

  icl_tasks:
  -
    label: piqa
    dataset_uri: s3://mosaicml-internal-dataset-hellaswag/piqa.jsonz
    num_fewshot:
    - 5
    batch_size: 16
    icl_task_type: multiple_choice
    metric_names:
    - InContextLearningMultipleChoiceAccuracy
    prompt_string: '' # this goes at the beginning of each input
    example_delimiter: '\n' # this goes between fewshot examples
    continuation_delimiter: ' ' # this separates questions from answers
  -
    label: lambada
    dataset_uri: s3://mosaicml-internal-dataset-lambda/lambada/lambada_openai.jsonl
    num_fewshot:
    - 0
    batch_size: 16
    icl_task_type: language_modeling
    metric_names:
    - InContextLearningLMAccuracy
    prompt_string: '' # this goes at the beginning of each input
    example_delimiter: '\n' # this goes between fewshot examples
    continuation_delimiter: '' # this separates contexts from continuations
